# -*- coding: utf-8 -*-
"""Event Classification using Claude.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15gmLQAc50odRz5Q5PID9h1usxOGMA9xR
"""

!pip install -q langchain langgraph pydantic langchain-community faiss-cpu \
               sentence-transformers langchain-anthropic

import os
import json
import csv
from typing import List, Literal, Optional
from typing_extensions import TypedDict
from pydantic import BaseModel, Field
from google.colab import files

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>> CHANGED IMPORTS <<<<<<<<<<<<<<<<<<<<<<<<<<<<
# LLM: Anthropic Claude
from langchain_anthropic import ChatAnthropic
# Local embeddings (no paid API needed)
from langchain_community.embeddings import HuggingFaceEmbeddings
# Vector store + docs + graph
from langchain.vectorstores.faiss import FAISS
from langchain.docstore.document import Document
from langgraph.graph import StateGraph, END
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# --- KEY SETUP ---
# Use env/Colab secrets (DON'T hardcode)
os.environ["ANTHROPIC_API_KEY"] = ""  # <-- set this securely (e.g., in Colab: Settings â†’ Secrets)

# --- 3) Schemas ---
class Event(BaseModel):
    id: str
    definition: str
    positives: List[str]
    negatives: List[str]

class LLMClassificationOutput(BaseModel):
    event_id: str = Field(description="The ID of the most likely event.")
    confidence: Literal["high", "medium", "low"] = Field(description="The confidence level of the classification.")
    rationale: str = Field(description="Brief rationale for why this event was chosen.")
    secondary_candidates: Optional[List[str]] = Field(
        description="Other plausible event IDs from the same catalog.", default=None
    )

# --- 4) Event Catalogs (your originals kept the same) ---
user_events_data = [
    {
        "id": "Candidate_Response_Event",
        "definition": "Candidate answers a question or provides relevant information about their background/skills/experience.",
        "positives": ["Iâ€™ve been a backend engineer", "I used Java and Python", "My experience includes"],
        "negatives": ["What do you mean by", "That wasnâ€™t my question"],
    },
    {
        "id": "Off_Topic_Detection_Event",
        "definition": "Candidate shifts the conversation away from the interview topic without clear relevance.",
        "positives": ["By the way did you watch the game", "Unrelated to the job"],
        "negatives": ["Background or relevant anecdote"],
    },
    {
        "id": "Candidate_Clarification_Request",
        "definition": "Candidate asks the interviewer to clarify or rephrase a prior question or statement.",
        "positives": ["Could you clarify", "What do you mean by", "Can you explain that again"],
        "negatives": ["Clarify that I used Java", "Let me clarify my past role"],
    },
    {
        "id": "Candidate_Closing_Response",
        "definition": "Candidate closes the conversation or acknowledges wrap-up (thanks, goodbyes, final remarks).",
        "positives": ["Thanks for your time", "That covers everything from my side", "Looking forward to next steps, bye"],
        "negatives": ["Thanks, and I also want to ask", "Goodbyeâ€”also one more thing"],
    },
]

agent_events_data = [
    {
        "id": "Greeting_Event",
        "definition": "Interviewer greets the candidate, sets a friendly tone, or welcomes them.",
        "positives": ["Hello John, thank you for joining", "Welcome to the interview"],
        "negatives": ["Technical question immediately"],
    },
    {
        "id": "Question_Asked_Event",
        "definition": "Interviewer asks a question, requests info, or prompts a response.",
        "positives": ["Could you start with a brief intro", "Tell me about a time", "Can you explain"],
        "negatives": ["Acknowledges answer", "Gives feedback rather than asks"],
    },
    {
        "id": "Follow_Up_Probe_Event",
        "definition": "Interviewer probes deeper on a previous response, asks for details or examples.",
        "positives": ["Can you go deeper", "Could you elaborate", "What specifically did you"],
        "negatives": ["Brand new topic question without reference"],
    },
    {
        "id": "Acknowledgement_Event",
        "definition": "Interviewer acknowledges or reflects back candidate's response without asking a new question.",
        "positives": ["Great â€” thanks for sharing", "Got it, that makes sense"],
        "negatives": ["Asks another question"],
    },
    {
        "id": "Technical_Assessment_Event",
        "definition": "Interviewer initiates or conducts technical evaluation (coding/architecture/scenario).",
        "positives": ["Letâ€™s do a coding exercise", "Design an API", "Whiteboard the architecture"],
        "negatives": ["Company culture question"],
    },
    {
        "id": "Cultural_Value_Assessment_Event",
        "definition": "Interviewer explores culture fit, values, collaboration, ownership, communication style, or work ethics.",
        "positives": ["How do you handle feedback", "Tell me about teamwork", "What values are important to you at work"],
        "negatives": ["Purely technical design question"],
    },
    {
        "id": "Feedback_or_Score_Event",
        "definition": "Interviewer provides evaluation, meta-feedback, or an explicit/implicit score during or after responses.",
        "positives": ["That's a strong answer", "I'd rate that solution as solid", "This meets our criteria"],
        "negatives": ["Asks for more info", "Describes company process without evaluating the candidate"],
    },
    {
        "id": "Agent_Rephrase_Event",
        "definition": "Interviewer rephrases or restates a previous question to improve clarity without adding a new topic.",
        "positives": ["Let me rephrase", "In other words", "To clarify, I'm asking"],
        "negatives": ["Introduces a new question/topic"],
    },
    {
        "id": "System_Error_Event",
        "definition": "Interviewer surfaces a system/tooling error, timeout, or failure message.",
        "positives": ["Iâ€™m seeing a system error", "The tool crashed", "I lost connection"],
        "negatives": ["Asks for more time", "General apology without error"],
    },
    {
        "id": "Latency_or_Pause_Event",
        "definition": "Interviewer exhibits a notable delay, hesitation, or filler indicating a pause rather than content.",
        "positives": ["...", "uh", "hmm", "(long pause)"],
        "negatives": ["Let's pause this interview", "Pause the service"],
    },
    {
        "id": "Closing_Event",
        "definition": "Interviewer wraps up or closes the session (thanks, goodbye, next steps).",
        "positives": ["Thanks for your time today", "That concludes our interview", "Next steps: our team will follow up"],
        "negatives": ["Thanksâ€”now one more question"],
    },
    {
        "id": "Off_Topic_Detection_Event",
        "definition": "Agent shifts the conversation away from the interview topic without clear relevance.",
        "positives": ["By the way did you watch the game", "Unrelated to the job"],
        "negatives": ["Background or relevant anecdote"],
    },
]

# Convert to models
user_events = [Event(**d) for d in user_events_data]
agent_events = [Event(**d) for d in agent_events_data]

# --- 5) Build separate vector stores for User vs Agent ---
def build_retriever(events: List[Event], k: int = 3):
    docs = []
    for ev in events:
        content = f"Event: {ev.id}\nDefinition: {ev.definition}\nExamples: {'; '.join(ev.positives)}"
        docs.append(Document(page_content=content, metadata={"id": ev.id}))

    # >>>>>>>>>>>>>>>>>>>> CHANGED: local HuggingFace embeddings <<<<<<<<<<<<<<<<
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

    store = FAISS.from_documents(docs, embeddings)
    return store.as_retriever(search_kwargs={"k": k})

user_retriever = build_retriever(user_events, k=4)
agent_retriever = build_retriever(agent_events, k=4)

# Helper maps for quick lookup
EVENTS_BY_ID = {e.id: e for e in (user_events + agent_events)}

# --- 6) LangGraph: choose candidate pool based on speaker ---
class GraphState(TypedDict):
    conversation: str   # recent context window
    speaker: str        # "User" or "Agent"
    candidate_events: List[Event]
    classification_result: LLMClassificationOutput

def retrieve_candidates(state: GraphState):
    print("--- STAGE A: RETRIEVE CANDIDATES ---")
    convo = state["conversation"]
    speaker = state["speaker"]
    retriever = user_retriever if speaker == "User" else agent_retriever
    results = retriever.invoke(convo)
    ids = [d.metadata["id"] for d in results]
    pool = [EVENTS_BY_ID[i] for i in ids if i in EVENTS_BY_ID]
    print(f"Speaker={speaker} | Candidates={ids}")
    return {"candidate_events": pool}

def classify_event(state: GraphState):
    print("--- STAGE B: CLASSIFY ---")
    convo = state["conversation"]
    candidates = state["candidate_events"]

    # Backstop: if recall returns nothing, fall back to full catalog for that speaker
    if not candidates:
        print("No candidates from retriever; falling back to full catalog for this speaker.")
        candidates = user_events if state["speaker"] == "User" else agent_events

    candidate_text = "\n\n".join(
        [
            f"ID: {e.id}\nDefinition: {e.definition}\nPositive Example: '{(e.positives or [''])[0]}'"
            for e in candidates
        ]
    )

    system_prompt = f"""
You are an expert interview conversation analyst. Classify the MOST RECENT message in the context
into exactly ONE of the candidate events for the correct speaker type.

Rules:
- Focus on the last message only; use previous turns for context.
- Choose ONE event from the provided candidate list (do not invent new).
- Output must match the provided JSON schema.

--- CANDIDATE EVENTS ---
{candidate_text}
"""

    # >>>>>>>>>>>>>>>>>>>>>>>>>>> CHANGED: Claude LLM <<<<<<<<<<<<<<<<<<<<<<<<<<
    llm = ChatAnthropic(model="claude-3-7-sonnet-20250219", temperature=0)
    structured = llm.with_structured_output(LLMClassificationOutput)
    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

    result = structured.invoke(
        [
            ("system", system_prompt),
            ("human", f"Conversation:\n---\n{convo}\n---")
        ]
    )
    print(f"Predicted: {result.event_id} ({result.confidence})")
    return {"classification_result": result}

workflow = StateGraph(GraphState)
workflow.add_node("retrieve_candidates", retrieve_candidates)
workflow.add_node("classify_event", classify_event)
workflow.set_entry_point("retrieve_candidates")
workflow.add_edge("retrieve_candidates", "classify_event")
workflow.add_edge("classify_event", END)
app = workflow.compile()

# --- 7) CSV Upload + Normalization ---
def normalize_speaker(raw: str) -> str:
    """Map raw 'Speaker' to 'User' or 'Agent'."""
    if not raw:
        return "User"
    s = raw.strip().lower()
    agent_aliases = {"ai_interviewer", "interviewer", "agent", "assistant", "system", "ai"}
    user_aliases  = {"candidate", "applicant", "user", "human"}
    if s in agent_aliases:
        return "Agent"
    if s in user_aliases:
        return "User"
    # Heuristic: if it mentions "interviewer" or "ai" treat as Agent, else User.
    if "interviewer" in s or "ai" in s or "assistant" in s:
        return "Agent"
    return "User"

print("ðŸ“¤ Please upload your chat CSV (columns: Timestamp, Speaker, Message_Text)")
uploaded = files.upload()
if not uploaded:
    raise RuntimeError("No file uploaded.")

csv_path = list(uploaded.keys())[0]
print(f"Reading: {csv_path}")

conversation = []  # list of dicts: {timestamp, speaker, text}
with open(csv_path, "r", encoding="utf-8-sig") as f:
    reader = csv.DictReader(f, delimiter=",")
    # Support tab-delimited too, if needed:
    if reader.fieldnames is None or len(reader.fieldnames) == 1:
        f.seek(0)
        reader = csv.DictReader(f, delimiter="\t")
    # Normalize expected fields
    fields = {name.lower(): name for name in (reader.fieldnames or [])}
    ts_key = fields.get("timestamp", "Timestamp")
    sp_key = fields.get("speaker", "Speaker")
    msg_key = fields.get("message_text", "Message_Text")

    for row in reader:
        timestamp = row.get(ts_key, "").strip()
        raw_speaker = row.get(sp_key, "").strip()
        text = row.get(msg_key, "").strip()
        if not text:
            continue
        conversation.append(
            {
                "timestamp": timestamp,
                "speaker": normalize_speaker(raw_speaker),
                "raw_speaker": raw_speaker,
                "text": text
            }
        )

print(f"Loaded {len(conversation)} messages.")

# --- 8) Run classification over the CSV ---
CONTEXT_WINDOW_SIZE = 8  # last N messages for context
OUTPUT_CSV = "classification_results.csv"
OUTPUT_HEADER = [
    "Timestamp",
    "Speaker",
    "Raw_Speaker",
    "Message_Text",
    "Event",
    "Confidence",
    "Rationale",
    "Secondary_Candidates"
]

with open(OUTPUT_CSV, "w", newline="", encoding="utf-8") as out:
    w = csv.writer(out)
    w.writerow(OUTPUT_HEADER)

    for i, turn in enumerate(conversation):
        speaker = turn["speaker"]
        # Build rolling context window up to and including this message
        start = max(0, i - CONTEXT_WINDOW_SIZE + 1)
        ctx_slice = conversation[start : i + 1]
        formatted_context = "\n".join([f"{t['speaker']}: {t['text']}" for t in ctx_slice])

        print("\n" + "="*70)
        print(f"Classifying message #{i+1} [{speaker}]: {turn['text']}")
        print("="*70)

        inputs = {"conversation": formatted_context, "speaker": speaker}
        result = app.invoke(inputs)["classification_result"]

        w.writerow([
            turn["timestamp"],
            speaker,
            turn["raw_speaker"],
            turn["text"],
            result.event_id,
            result.confidence,
            result.rationale,
            "; ".join(result.secondary_candidates or [])
        ])

print(f"\nâœ… Done. Wrote: {OUTPUT_CSV}")
files.download(OUTPUT_CSV)